{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtQ8g7jvO41exxV4B9MyzL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisabecker/nlp-fundamentals/blob/main/0504_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs)\n",
        "\n",
        "This section provides an overview of what LLMs are and introduces the concept of Retrieval-Augmented Generation (RAG)."
      ],
      "metadata": {
        "id": "v4Bu8m0N5WTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setting up the environment"
      ],
      "metadata": {
        "id": "nFMCzUQBA1yQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-8lFIU_jsgN"
      },
      "outputs": [],
      "source": [
        "# Install the necessary packages\n",
        "!python3 -m pip install --q transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploring a Pre-Trained Large Language Model (LLM)\n",
        "\n",
        "In this section, we initialize and explore the capabilities of GPT-2, a pre-trained Large Language Model (LLM). GPT-2 is well-known for its ability to generate coherent and contextually relevant text.\n",
        "\n",
        "First, we set up the GPT-2 tokenizer and model. The tokenizer converts our input text into a format that the model can understand (tokenization), and the model is then used to generate a text response.\n",
        "\n",
        "We also include a special configuration to use the end-of-sequence token as a padding token. This adjustment is necessary because GPT-2 does not have a default padding token, and padding is crucial for handling variable-length inputs.\n",
        "\n",
        "The function `generate_text` is defined to encapsulate the text generation process. It takes a prompt as input, tokenizes it, generates a response using GPT-2, and then decodes this response back into human-readable text.\n",
        "\n",
        "![LLM Visualisation](https://github.com/lisabecker/nlp-fundamentals/blob/main/graphics/llm.png?raw=true)"
      ],
      "metadata": {
        "id": "OrIAAU1_86MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Initialize GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Function to generate text using GPT-2\n",
        "def generate_text(prompt, max_length=50):\n",
        "    # Encode the input with attention mask\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length)\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "\n",
        "    # Generate response using the model\n",
        "    output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length)\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Queen Elizabeth II died on\"\n",
        "response = generate_text(prompt)\n",
        "print(f\"\\nGPT2 Response: '{response}...'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laLhyEDuTPea",
        "outputId": "eb487f48-03ee-4405-ccdd-ed0d07e63cfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT2 Response: 'Queen Elizabeth II died on the 15th of July, 1714, at the age of sixty-five.\n",
            "\n",
            "The first of the three daughters of the Queen Elizabeth II, Elizabeth, was born in 1714, and was the daughter of the...'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) combines the power of language models with external knowledge retrieval to generate responses that are not only contextually relevant but also factually accurate. In this section, we will create a simple RAG setup.\n",
        "\n",
        "![RAG Visualisation](https://github.com/lisabecker/nlp-fundamentals/blob/main/graphics/rag.png?raw=true)\n",
        "\n",
        "### Dataset Definition\n",
        "\n",
        "We start by defining a simple Q&A dataset. This dataset is a collection of question-answer pairs where each question has a corresponding factual answer. The dataset is represented as a dictionary where questions are keys and answers are values.\n",
        "\n",
        "This dataset serves as our knowledge base, from which the model will retrieve information to augment its responses.\n"
      ],
      "metadata": {
        "id": "isNf1A6wyAQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your dataset\n",
        "qa_dataset_dict = {\n",
        "    \"When did Queen Elizabeth II. die?\": \"Queen Elizabeth II. died on September 8th 2022.\",\n",
        "    \"Who created the 'Fundamentals of Natural Language Processing' course for O'Reilly?\": \"Lisa Becker.\",\n",
        "    \"Who won the 2022 FIFA World Cup?\": \"Argentina.\",\n",
        "    # Add more Q&A pairs as needed!\n",
        "}\n",
        "\n",
        "# Extracting questions and answers\n",
        "questions = list(qa_dataset_dict.keys())\n",
        "answers = list(qa_dataset_dict.values())\n",
        "print(f\"Questions: {questions}\")\n",
        "print(f\"Answers: {answers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-mMKajpW31l",
        "outputId": "9cd77b62-a55f-43aa-fe50-a0394aad5917"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions: ['When did Queen Elizabeth II. die?', \"Who created the 'Fundamentals of Natural Language Processing' course for O'Reilly?\", 'Who won the 2022 FIFA World Cup?']\n",
            "Answers: ['Queen Elizabeth II. died on September 8th 2022.', 'Lisa Becker.', 'Argentina.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Embedding Model\n",
        "\n",
        "To enable our RAG setup to retrieve relevant information from our Q&A dataset, we need to convert our questions into a format that allows for similarity comparison. This process is known as embedding.\n",
        "\n",
        "We use the `SentenceTransformer` model to generate embeddings for each question in our dataset. These embeddings are high-dimensional vector representations that capture the semantic meaning of the questions.\n",
        "\n",
        "Once we have these embeddings, we can compare any new incoming question's embedding with them to find the most semantically similar existing question."
      ],
      "metadata": {
        "id": "Tju4G1YwzBxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize a sentence transformer model for embeddings\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract questions from the dataset\n",
        "questions = list(qa_dataset_dict.keys())\n",
        "\n",
        "# Embedding questions\n",
        "question_embeddings = embedder.encode(questions)"
      ],
      "metadata": {
        "id": "sUuBKgLhX68n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Question: {list(qa_dataset_dict.keys())[0]}\")\n",
        "print(f\"Embedding dimentions: {len(question_embeddings[0])}\")\n",
        "print(f\"Embeddings:\\n {question_embeddings[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97g84d08dPbT",
        "outputId": "1037ec5b-14bd-45e3-bcc1-c2f768c7432d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When did Queen Elizabeth II. die?\n",
            "Embedding dimentions: 384\n",
            "Embeddings:\n",
            " [ 5.65683655e-02 -1.36119397e-02  9.75994393e-03  6.66206256e-02\n",
            "  1.73170771e-02  7.42301857e-03  1.11427810e-03 -1.52328601e-02\n",
            " -5.11999205e-02  4.05504145e-02 -7.08348826e-02 -9.96009167e-03\n",
            " -2.76151896e-02 -6.15489483e-02 -7.34817935e-03  3.64168398e-02\n",
            " -1.82406902e-02 -1.01685338e-02 -6.06067143e-02  4.16703038e-02\n",
            "  3.53916809e-02  1.46008953e-02  4.23478410e-02  6.47900179e-02\n",
            " -1.80922579e-02 -4.05573919e-02 -9.04035941e-02  5.66098392e-02\n",
            "  2.46191192e-02 -4.26385589e-02 -5.78839742e-02 -6.97087497e-02\n",
            " -6.07863590e-02  2.82646026e-02  1.19467815e-02 -9.81372688e-03\n",
            "  4.25702818e-02  4.75785695e-02 -1.67285465e-02 -4.29072306e-02\n",
            " -6.23256750e-02 -2.55048871e-02  7.53114000e-03  3.82316783e-02\n",
            "  4.61017787e-02  7.28343846e-03 -3.10446993e-02  6.07980192e-02\n",
            "  2.05025449e-02 -9.08780005e-03  3.26228957e-03  3.46131041e-03\n",
            "  2.20972463e-03 -7.34083578e-02  2.35510189e-02 -1.40735703e-02\n",
            " -7.02478811e-02 -3.11216037e-03  9.93422791e-02 -5.87139428e-02\n",
            "  6.66985428e-03  6.63153380e-02  2.23306883e-02  3.67415808e-02\n",
            " -2.43698750e-02 -5.69998706e-03  3.98501866e-02 -1.03654109e-01\n",
            "  3.55011411e-02  1.28945522e-02 -4.83899787e-02 -8.48237276e-02\n",
            "  2.15514526e-02 -7.15036467e-02 -7.81861246e-02  2.62594018e-02\n",
            "  9.37240720e-02  3.10196411e-02 -1.04023062e-01  5.48150800e-02\n",
            " -7.87833109e-02 -3.52950245e-02  8.93355981e-02  1.04728796e-01\n",
            "  7.08919093e-02  4.54178173e-03 -2.23135762e-02 -1.10397004e-02\n",
            " -3.77225876e-02 -2.28873733e-02  2.47643460e-02 -5.05204052e-02\n",
            "  7.42973164e-02  8.39298218e-02  3.78104742e-03  5.37829939e-03\n",
            " -8.69447738e-03  4.14321752e-04  4.91454303e-02  3.88909988e-02\n",
            " -6.47777468e-02 -2.78470777e-02 -4.38297726e-02  4.68609072e-02\n",
            " -2.64488999e-02  3.18449475e-02  2.54091676e-02 -2.25115847e-02\n",
            "  4.76427749e-02 -3.21106575e-02 -2.01229006e-02 -3.47827259e-03\n",
            " -3.92589113e-03  2.45222971e-02  6.52858689e-02  2.04533841e-02\n",
            " -4.35468834e-03  1.97058194e-03  3.44638936e-02 -3.17564011e-02\n",
            "  5.68288714e-02 -9.19637270e-03 -4.09381203e-02 -6.12917878e-02\n",
            " -5.52107431e-02  2.68914290e-02  1.48846917e-02 -3.43820710e-33\n",
            " -4.09104675e-02 -5.25935851e-02  4.68914956e-02  1.07417956e-01\n",
            "  4.55898009e-02  7.81012932e-04 -9.17975733e-04  5.14004268e-02\n",
            "  7.27354214e-02  2.39124079e-03  2.48683468e-02 -1.19461417e-01\n",
            " -2.62420066e-02 -1.81320027e-01 -1.09406393e-02  2.50471886e-02\n",
            "  3.72389593e-04  2.62573417e-02  5.70507497e-02 -8.96871556e-03\n",
            " -1.83174256e-02  1.67692248e-02  5.40325008e-02  4.37476747e-02\n",
            "  1.79057773e-02  3.47881727e-02  9.48878601e-02 -3.58190425e-02\n",
            "  3.42888311e-02  4.02608812e-02  7.13637620e-02 -7.54173622e-02\n",
            " -4.31603007e-02 -5.92507236e-02  1.52825657e-02  2.73124985e-02\n",
            " -1.03568677e-02 -3.07644252e-02  6.93528494e-03  4.70865369e-02\n",
            " -1.13093583e-02 -3.55673544e-02  1.17407657e-01 -8.68453353e-04\n",
            " -2.74459161e-02 -9.58409235e-02  7.41356760e-02  1.66168548e-02\n",
            "  8.42517093e-02 -1.80933829e-02  3.07441875e-02 -4.49218005e-02\n",
            " -9.59526449e-02  2.18919907e-02  6.22038823e-03  1.02555409e-01\n",
            " -6.09180424e-03  2.88576307e-03  7.96806663e-02  7.69015551e-02\n",
            "  6.21087663e-02  4.55595702e-02  1.43347271e-02  4.85336455e-03\n",
            " -3.12267896e-02  6.26806691e-02 -1.05289780e-02 -6.91989139e-02\n",
            " -4.74878401e-02 -6.55575618e-02  5.77144604e-03  3.25199589e-02\n",
            "  4.45766486e-02 -5.58830462e-02 -2.67112814e-02  1.72738694e-02\n",
            " -1.72396433e-02 -1.00950360e-01  7.26252934e-03  3.79417576e-02\n",
            " -1.44290170e-02  1.08641423e-02  3.98158655e-02  5.19083552e-02\n",
            "  4.67002094e-02 -1.14311121e-01 -1.81791969e-02 -1.17419222e-02\n",
            "  1.06807835e-02  3.71223912e-02 -6.38110712e-02  1.81649830e-02\n",
            "  3.89371323e-03 -5.15262224e-02 -8.84282961e-02  1.06384369e-33\n",
            " -2.76567806e-02 -4.80308291e-03  4.96558994e-02  1.05432786e-01\n",
            " -5.07197110e-04 -4.56323326e-02 -6.15300089e-02  6.29369020e-02\n",
            "  2.42977850e-02 -2.54314523e-02  8.82925987e-02  8.55346862e-03\n",
            "  4.24758904e-02  8.47417712e-02 -2.04955377e-02  2.64868010e-02\n",
            " -1.02150347e-02 -6.86355978e-02 -7.69814057e-03 -4.89140861e-02\n",
            " -1.15465395e-01 -5.08052185e-02  1.71442106e-02 -3.43553200e-02\n",
            "  4.54245247e-02  9.88168940e-02 -4.15936708e-02 -3.84966172e-02\n",
            " -3.05056274e-02 -5.78379706e-02  2.07021236e-02  7.25487843e-02\n",
            " -6.48751706e-02 -2.42312886e-02 -1.14977816e-02  1.93682406e-02\n",
            "  9.42567084e-03 -4.25536409e-02  3.16245891e-02  4.35587168e-02\n",
            " -3.83880548e-02 -1.78984404e-02  6.91961348e-02  9.52506363e-02\n",
            "  6.53189197e-02 -3.31898406e-02 -7.15184258e-03  5.43021969e-02\n",
            "  1.63663283e-01  3.60736847e-02 -5.75731955e-02 -1.83044299e-02\n",
            "  3.27340327e-02  4.76566292e-02  2.59098988e-02 -5.95920812e-03\n",
            " -2.78315898e-02  4.10594465e-03  1.03593972e-02 -5.08449674e-02\n",
            "  5.41750304e-02 -3.36404108e-02 -3.22786458e-02 -3.28051089e-03\n",
            " -8.13731551e-03  7.93844014e-02  2.97915679e-03  3.43785100e-02\n",
            " -2.60856524e-02  1.47089055e-02  7.55126402e-02  1.41427740e-02\n",
            " -6.38395920e-02  2.00998178e-03 -1.06193833e-01  1.98087264e-02\n",
            " -2.70240158e-02 -5.24814241e-02 -3.80204362e-03 -1.89909395e-02\n",
            " -9.65698287e-02  2.19015349e-02 -2.69392245e-02 -4.13625315e-02\n",
            " -5.34382649e-02 -8.23122188e-02  3.58458236e-02  1.50651485e-02\n",
            " -3.88077274e-02 -1.33474737e-01 -4.84034792e-02 -5.46489321e-02\n",
            "  4.87498529e-02 -8.53085592e-02  2.63909418e-02 -1.40677550e-08\n",
            "  3.97040360e-02  7.61145726e-02 -7.18011521e-03 -1.06508307e-01\n",
            "  3.45695973e-03 -3.71654741e-02  1.63657777e-02  1.00548543e-01\n",
            "  2.53871121e-02 -9.37775988e-03 -5.63509837e-02  3.18833888e-02\n",
            "  3.59131061e-02 -1.23253241e-02  7.69853666e-02  4.38706996e-03\n",
            " -4.57222089e-02 -6.72966093e-02  2.35376190e-02  1.27484761e-02\n",
            "  1.53199667e-02 -5.91421425e-02  3.46920826e-02 -7.94167891e-02\n",
            " -9.63534520e-04 -3.50702517e-02  4.61600907e-02  1.84286926e-02\n",
            " -4.90573347e-02  4.08979505e-03 -1.23585155e-02  5.51321059e-02\n",
            " -8.34034849e-03 -3.95516604e-02 -8.37780461e-02 -1.69448275e-02\n",
            "  1.01402894e-01 -3.49469506e-03 -6.82105795e-02 -6.80159852e-02\n",
            " -1.59713272e-02  1.43399507e-01 -3.35006453e-02  2.06423346e-02\n",
            "  1.34918079e-01 -5.67663684e-02  2.54073902e-03  5.54433651e-02\n",
            " -6.38994500e-02 -1.08280562e-01  3.38863544e-02  8.03821236e-02\n",
            "  4.42018732e-02  8.97049084e-02 -3.46586816e-02 -2.77363695e-02\n",
            " -2.84470785e-02 -3.34058367e-02 -7.71396607e-02  5.71072102e-02\n",
            "  2.98004523e-02 -3.64598744e-02 -2.05617491e-02 -2.88517792e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Cosine Similarity\n",
        "\n",
        "With our questions embedded, the next step is to embed an incoming question and compare it to our dataset. We use cosine similarity for this comparison.\n",
        "\n",
        "Cosine similarity measures the cosine of the angle between two vectors, in our case, the embedding of the incoming question and each question in our dataset. This metric helps us determine which question in our dataset is most similar to the incoming one.\n",
        "\n",
        "We print the cosine similarity scores for illustration, showing how similar each dataset question is to the incoming question.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Cbr8bxxCxh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Cosine Similarity Visualisation](https://github.com/lisabecker/nlp-fundamentals/blob/main/graphics/cosine_similarity.png?raw=true)"
      ],
      "metadata": {
        "id": "pqwsr2Hvx0P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Embed an incoming question\n",
        "incoming_question = \"Queen Elizabeth II died on\"\n",
        "incoming_question_embedding = embedder.encode([incoming_question])\n",
        "print(f\"{incoming_question}\\n\")\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_similarities = cosine_similarity(incoming_question_embedding, question_embeddings)\n",
        "for index, q in enumerate(questions):\n",
        "  print(f\"Question: {q}\\nCosine Similarity: {cos_similarities[0][index]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5DAhxvf8Ykr",
        "outputId": "5deccd09-ebe1-4947-b8cc-58ac767be4c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queen Elizabeth II died on\n",
            "\n",
            "Question: When did Queen Elizabeth II. die?\n",
            "Cosine Similarity: 0.8671935796737671\n",
            "\n",
            "Question: Who created the 'Fundamentals of Natural Language Processing' course for O'Reilly?\n",
            "Cosine Similarity: -0.068804070353508\n",
            "\n",
            "Question: Who won the 2022 FIFA World Cup?\n",
            "Cosine Similarity: 0.14708779752254486\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Most Relevant Q&A Pair\n",
        "\n",
        "After calculating the cosine similarities, we identify the question in our dataset that has the highest similarity score with the incoming question. This is our most relevant question, and its corresponding answer is likely to contain pertinent information related to the incoming question.\n",
        "\n",
        "We then extract this most relevant question and its answer from our dataset, as they will be used to generate an informed response.\n"
      ],
      "metadata": {
        "id": "NwhxVTtN5Ybj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the index of the most relevant question\n",
        "most_relevant_idx = np.argmax(cos_similarities)\n",
        "\n",
        "# Extract the most relevant question and its answer from your dictionary\n",
        "relevant_question = questions[most_relevant_idx]\n",
        "relevant_answer = answers[most_relevant_idx]\n",
        "print(f\"Relevant question: {relevant_question}\")\n",
        "print(f\"Relevant answer: {relevant_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgNE_P1zi9VX",
        "outputId": "f351f2ba-a883-4924-9f31-4f7d2f7e12b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant question: When did Queen Elizabeth II. die?\n",
            "Relevant answer: Queen Elizabeth II. died on September 8th 2022.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Generate Answer with Additional Knowledge\n",
        "\n",
        "Finally, we combine the answer to the most relevant question from our dataset with the incoming question. This combined input is then fed into the GPT-2 model to generate a response.\n",
        "\n",
        "By doing this, we leverage the retrieved factual information (answer from the dataset) and the contextual understanding of the LLM (GPT-2) to create a response that is both relevant and informed by external knowledge. This process exemplifies a simple yet effective RAG system."
      ],
      "metadata": {
        "id": "W0n3U3U3zLaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the relevant answer and incoming question\n",
        "combined_input = relevant_answer + \" \" + incoming_question\n",
        "\n",
        "# Generate a response using the LLM\n",
        "extended_response = generate_text(combined_input)\n",
        "print(extended_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXdF0JU5P0se",
        "outputId": "0104e10f-789a-491d-ac7f-b877f37063c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queen Elizabeth II. died on September 8th 2022. Queen Elizabeth II died on September 8th 2022.\n",
            "\n",
            "The Queen Elizabeth II. died on September 8th 2022. Queen Elizabeth II died on September 8th 2022.\n",
            "\n",
            "The Queen Elizabeth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Additional Resources\n",
        "\n",
        "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
        "- [Sentence-Transformers](https://www.sbert.net/)\n",
        "- [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)"
      ],
      "metadata": {
        "id": "UtnngSJC9CvR"
      }
    }
  ]
}